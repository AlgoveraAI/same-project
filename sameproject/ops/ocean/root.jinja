{% autoescape off %}

import matplotlib.pyplot as plt
import numpy as np
import os 
from pathlib import Path
import pickle
import random
import sys

import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils

def root(
    {{ root_parameters_as_string }},
	context="",
	metadata_url="",
):
    # The below is base64 encoding of an empty locals() output
	__original_context = ""
	if context == '':
		__original_context = "gAR9lC4="
	else:
		__original_context = context

    experiment = Experiment(ws, "{{ experiment_name }}")

    run_info_dict = {
		"experiment_id": experiment.id,
		"step_id": "run_info_step",
	}

    output = {}
	output["run_info"] = str(
		base64.urlsafe_b64encode(dill.dumps(run_info_dict)), encoding="ascii"
	)

{% for step in list_of_steps %}
	entry_point = "{{step.unique_name}}.py"
	__pipelinedata_context_{{step.unique_name}} = PipelineData(
		"__pipelinedata_context_{{step.unique_name}}", output_mode="mount"
	)

	{{step.unique_name}}_step = PythonScriptStep(
		source_directory="{{compile_path}}/{{step.unique_name}}",
		script_name=entry_point,
		arguments=[
			"--input_context",
			{% if step.previous_step_name %}__pipelinedata_context_{{step.previous_step_name}}{% else %}__original_context_param{% endif %},
			"--run_info",
			output["run_info"],
			"--metadata_url",
			metadata_url,
			"--output_context",
			__pipelinedata_context_{{step.unique_name}},
		],

		{% if step.previous_step_name %}inputs=[__pipelinedata_context_{{step.previous_step_name}}],{% endif %}
		outputs=[__pipelinedata_context_{{step.unique_name}}],
		compute_target=compute_target,
		runconfig=config_{{step.environment_name}},
		allow_reuse=False,
		)

{% endfor %}

	run_pipeline_definition = [{{comma_delim_list_of_step_names_as_str}}]

	built_pipeline = Pipeline(workspace=ws, steps=[run_pipeline_definition])
	pipeline_run = experiment.submit(built_pipeline)


def test_dcgan(local=False):

    results_dir = Path('results')

    if not results_dir.exists():
        results_dir.mkdir()

    weights_path = Path("netG.pth")

    if not weights_path.exists():
        os.system("wget https://www.dropbox.com/s/p3pjgmpiki7w0ur/netG.pth")

    nc = 3
    nz = 100
    ngf = 64
    ngpu = 1

    # Decide which device we want to run on
    device = torch.device("cuda:0" if (torch.cuda.is_available() and ngpu > 0) else "cpu")

    # Network
    network = {% network %}

    # Create the generator
    netG = network(ngpu).to(device)

    # Handle multi-gpu if desired
    if (device.type == 'cuda') and (ngpu > 1):
        netG = nn.DataParallel(netG, list(range(ngpu)))

    netG.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu')))

    netG.eval()

    # Create batch of latent vectors that we will use to visualize
    #  the progression of the generator
    fixed_noise = torch.randn(1, nz, 1, 1, device=device)

    with torch.no_grad():
        img = netG(fixed_noise).detach().cpu()

    def denormalize_image(image):
        """Reverse to normalize_image() function"""
        max_ = image.max()
        min_ = image.min()
        return (image - min_)/(max_ - min_)

    img = denormalize_image(img)

    img = img.squeeze().permute(1,2,0)

    filename = results_dir / 'test.pickle' if local else "/data/outputs/result"

    with open(filename, 'wb') as pickle_file:
        print(f"Pickling results in {filename}")
        pickle.dump(img, pickle_file)

if __name__ == "__main__":
    local = (len(sys.argv) == 2 and sys.argv[1] == "local")
    test_dcgan(local)

	# execute only if run as a script
	root(
		context="gAR9lC4=", metadata_url=""
	)
{% endautoescape %}